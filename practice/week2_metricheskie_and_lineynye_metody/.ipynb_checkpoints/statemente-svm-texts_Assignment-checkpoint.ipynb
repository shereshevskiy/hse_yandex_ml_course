{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание по программированию: Анализ текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вы научитесь:\n",
    "- находить оптимальные параметры для метода опорных векторов\n",
    "- работать с текстовыми данными\n",
    "\n",
    "## Введение\n",
    "Метод опорных векторов (Support Vector Machine, SVM) — один из видов линейных классификаторов. Функционал, который он оптимизирует, направлен на максимизацию ширины разделяющей полосы между классами. Из теории статистического обучения известно, что эта ширина тесно связана с обобщающей способностью алгоритма, а ее максимизация позволяет бороться с переобучением.\n",
    "\n",
    "Одна из причин популярности линейных методов заключается в том, что они хорошо работают на **разреженных данных**. Так называются выборки с большим количеством признаков, где на каждом объекте большинство признаков равны нулю. Разреженные данные возникают, например, при работе с текстами. Дело в том, что текст удобно кодировать с помощью \"мешка слов\" — формируется столько признаков, сколько всего уникальных слов встречается в текстах, и значение каждого признака равно числу вхождений в документ соответствующего слова. Ясно, что общее число различных слов в наборе текстов может достигать десятков тысяч, и при этом лишь небольшая их часть будет встречаться в одном конкретном тексте.\n",
    "\n",
    "Можно кодировать тексты хитрее, и записывать не количество вхождений слова в текст, а <a href='https://ru.wikipedia.org/wiki/TF-IDF'>TF-IDF</a>. Это показатель, который равен произведению двух чисел: `TF (term frequency)` и `IDF (inverse document frequency)`. Первая равна отношению числа вхождений слова в документ к общей длине документа. Вторая величина зависит от того, в скольки документах выборки встречается это слово. Чем больше таких документов, тем меньше IDF. Таким образом, **TF-IDF** будет иметь высокое значение для тех слов, которые **много раз встречаются в данном документе, и редко встречаются в остальных**.\n",
    "\n",
    "## Данные\n",
    "Как мы уже говорили выше, линейные методы часто применяются для решения различных задач анализа текстов. В этом задании мы применим метод опорных векторов для определения того, к какой из тематик относится новость: атеизм или космос.\n",
    "\n",
    "## Реализация в Scikit-Learn\n",
    "Для начала вам потребуется загрузить данные. В этом задании мы воспользуемся одним из датасетов, доступных в scikit-learn'е — **20 newsgroups**. Для этого нужно воспользоваться модулем `datasets`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "newsgroups = datasets.fetch_20newsgroups(\n",
    "                    subset='all', \n",
    "                    categories=['alt.atheism', 'sci.space']\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После выполнения этого кода массив с текстами будет находиться в поле `newsgroups.data`, номер класса — в поле `newsgroups.target`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из сложностей работы с текстовыми данными состоит в том, что для них нужно построить числовое представление. Одним из способов нахождения такого представления является вычисление `TF-IDF`. В Scikit-Learn это реализовано в классе `sklearn.feature_extraction.text.TfidfVectorizer`. Преобразование обучающей выборки нужно делать с помощью функции `fit_transform`, тестовой — с помощью `transform`.\n",
    "\n",
    "Реализация SVM-классификатора находится в классе `sklearn.svm.SVC`. Веса каждого признака у обученного классификатора хранятся в поле `coef_`. Чтобы понять, какому слову соответствует i-й признак, можно воспользоваться методом `get_feature_names()` у `TfidfVectorizer`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "feature_mapping = vectorizer.get_feature_names()\n",
    "print feature_mapping[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подбор параметров удобно делать с помощью класса sklearn.grid_search.GridSearchCV (При использовании библиотеки scikit-learn версии 18.0.1 sklearn.model_selection.GridSearchCV). Пример использования:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "grid = {'C': np.power(10.0, np.arange(-5, 6))}\n",
    "cv = KFold(y.size, n_folds=5, shuffle=True, random_state=241)\n",
    "clf = svm.SVC(kernel='linear', random_state=241)\n",
    "gs = GridSearchCV(clf, grid, scoring='accuracy', cv=cv)\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании библиотеки scikit-learn версии 18.0.1 и выше KFold задаётся немного по-другому:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=241)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первым аргументом в GridSearchCV передается классификатор, для которого будут подбираться значения параметров, вторым — словарь (dict), задающий сетку параметров для перебора. После того, как перебор окончен, можно проанализировать значения качества для всех значений параметров и выбрать наилучший вариант:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for a in gs.grid_scores_:\n",
    "    # a.mean_validation_score — оценка качества по кросс-валидации\n",
    "    # a.parameters — значения параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инструкция по выполнению\n",
    "1) Загрузите объекты из новостного датасета 20 newsgroups, относящиеся к категориям \"космос\" и \"атеизм\" (инструкция приведена выше). Обратите внимание, что загрузка данных может занять несколько минут"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# функция для записи ответов\n",
    "def write_answer(name, answer):\n",
    "    with open('data/' + name + '.txt', 'w') as file:\n",
    "        file.write(str(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups = datasets.fetch_20newsgroups(\n",
    "                    subset='all', \n",
    "                    categories=['alt.atheism', 'sci.space']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: 9051467f@levels.unisa.edu.au (The Desert Brat)\\nSubject: Re: Keith Schneider - Stealth Poster?\\nOrganization: Cured, discharged\\nLines: 24\\n\\nIn article <1pa0f4INNpit@gap.caltech.edu>, keith@cco.caltech.edu (Keith Allan Schneider) writes:\\n\\n> But really, are you threatened by the motto, or by the people that use it?\\n\\nEvery time somone writes something and says it is merely describing the norm,\\nit is infact re-inforcing that norm upon those programmed not to think for\\nthemselves. The motto is dangerous in itself, it tells the world that every\\n*true* American is god-fearing, and puts down those who do not fear gods. It\\ndoesn\\'t need anyone to make it dangerous, it does a good job itself by just\\nexisting on your currency.\\n\\n> keith\\n\\nThe Desert Brat\\n-- \\nJohn J McVey, Elc&Eltnc Eng, Whyalla, Uni S Australia,    ________\\n9051467f@levels.unisa.edu.au      T.S.A.K.C.            \\\\/Darwin o\\\\\\nFor replies, mail to whjjm@wh.whyalla.unisa.edu.au      /\\\\________/\\nDisclaimer: Unisa hates my opinions.                       bb  bb\\n+------------------------------------------------------+-----------------------+\\n|\"It doesn\\'t make a rainbow any less beautiful that we | \"God\\'s name is smack  |\\n|understand the refractive mechanisms that chance to   | for some.\"            |\\n|produce it.\" - Jim Perry, perry@dsinc.com             |    - Alice In Chains  |\\n+------------------------------------------------------+-----------------------+\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " newsgroups.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.target[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = newsgroups.data\n",
    "y = newsgroups.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Вычислите TF-IDF-признаки для всех текстов. Обратите внимание, что в этом задании мы предлагаем вам вычислить TF-IDF по **всем** данным. При таком подходе получается, что признаки на обучающем множестве используют информацию из тестовой выборки — но такая ситуация вполне законна, поскольку мы не используем значения целевой переменной из теста. На практике нередко встречаются ситуации, когда признаки объектов тестовой выборки известны на момент обучения, и поэтому можно ими пользоваться при обучении алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1786x28382 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 303138 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Подберите минимальный лучший параметр C из множества [10^-5, 10^-4, ... 10^4, 10^5] для SVM с линейным ядром (kernel='linear') при помощи кросс-валидации по 5 блокам. Укажите параметр random_state=241 и для SVM, и для KFold. В качестве меры качества используйте долю верных ответов (accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid = {'C': np.power(10.0, np.arange(-5, 6))}\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=241)\n",
    "clf = svm.SVC(kernel='linear', random_state=241)\n",
    "gs = GridSearchCV(clf, grid, scoring='accuracy', cv=cv)\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Обучите SVM по всей выборке с оптимальным параметром C, найденным на предыдущем шаге."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=241, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# это просто лучший определенный на предыдущем шаге классификаор\n",
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Найдите 10 слов с наибольшим абсолютным значением веса (веса хранятся в поле coef_ у svm.SVC). Они являются ответом на это задание. Укажите эти слова через запятую или пробел, в нижнем регистре, в лексикографическом порядке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** \"вытащим\" массив коэффициентов и преобразуем его из формата sparse в обычный массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.29258057, -0.12314757,  0.        , ...,  0.01972862,\n",
       "         0.05831336, -0.00297347]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_.coef_.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** отсортируем **минус** модули коэффициэнтов (\"минус\" - для сортировки по убыванию), возьмем их индексы (функция `np.argsort`) и выделим первые 10 индексов для перевода их в слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24019, 12871,  5088,  5093, 17802, 23673, 21850,  5776, 15606, 22936], dtype=int64)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word10_index = np.argsort(-np.abs(gs.best_estimator_.coef_.toarray()))[0, :10]\n",
    "word10_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "для примера попробуем применения преобразования индекса слова в слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article\n"
     ]
    }
   ],
   "source": [
    "feature_mapping = vectorizer.get_feature_names()\n",
    "print(feature_mapping[4890])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** преобразуеем индексы в слова и отсортируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atheism',\n",
       " 'atheists',\n",
       " 'bible',\n",
       " 'god',\n",
       " 'keith',\n",
       " 'moon',\n",
       " 'religion',\n",
       " 'sci',\n",
       " 'sky',\n",
       " 'space']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word10 = sorted([feature_mapping[ind] for ind in word10_index])\n",
    "word10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_answer('svm-texts_answer1', ' '.join(word10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
